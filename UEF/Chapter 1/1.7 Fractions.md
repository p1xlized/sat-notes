---
sticker: emoji//1f370
tags: uef, chapter1
banner: Attachments/covers/DALL·E 2025-03-09 21.06.13 - A detailed digital illustration explaining floating-point representation using a fraction. The image should depict a breakdown of a floating-point num.webp
banner_y: "56.5"
---
### Floating-Point Notation
Storing of decimal numbers requires a fractional part
A floating-point number consists of:
- **Sign bit** (1 bit): Determines whether the number is positive or negative.
  - `0` → Non-negative
  - `1` → Negative
- **Exponent** (3 bits): Encodes the exponent using an **[[UEF/Chapter 1/1.6 Storing integers.md|excess]]-2** notation.
- **Mantissa** (4 bits): Represents the fractional part, normalized to start with the left-most <font color="#ff0000">1</font>.
> [!NOTE]
    > Finite storage limits precision, so approximations are common.
    > 
### Example 2: Converting 5.5 to Floating-Point
1. Convert **5.5** to binary:  
   - $5.5_{10} = 101.1_2$
1. Normalize:  
   - $101.1_{2} → 1.011 × 2^2$
1. Store in floating-point format:
   - **Sign:** `0` (positive)
   - **Exponent:** `2` → **[[UEF/Chapter 1/1.6 Storing integers.md|Excess]]-2 encoding:** `010`
   - **Mantissa:** `0110`
**Resulting 8-[[UEF/Chapter 1/1.1 Bits & Their Storage.md|bit]] representation:** `00100110`
### Example 3: Converting -3.25 to Floating-Point
1. Convert **3.25** to binary:  
   - `3.25₁₀ = 11.01₂`
2. Normalize:  
   - `11.01₂ → 1.101 × 2¹`
3. Store in floating-point format:
   - **Sign:** `1` (negative)
   - **Exponent:** `1` → **Excess-2 encoding:** `011`
   - **Mantissa:** `1010`
**Resulting 8-[[UEF/Chapter 1/1.1 Bits & Their Storage.md|bit]] representation:** `10111010`
### **Summary Table**
| Decimal Value | Binary Representation | Normalized Form | Floating-Point (8-bit) |
| ------------- | --------------------- | --------------- | ---------------------- |
| 5.5           | `101.1`               | `1.011 × 2²`    | `00100110`             |
| -3.25         | `11.01`               | `1.101 × 2¹`    | `10111010`             |

![[Pasted image 20250309205710.png]]
Floating-point numbers have **limited precision**, causing **truncation** and **round-off errors** when storing numbers.
- **Truncation Error**: Cuts off extra digits without rounding.
    - Example: Storing π at 4 decimals → `3.1415` (error = `0.000092`)
- **Round-off Error**: Approximates to the nearest representable value.
    - Example: Storing π at 4 decimals → `3.1416` (error = `-0.000007`)
Some numbers **cannot be represented exactly** in binary, like **1/3 (0.333...)** or **1/10 (0.1)**, leading to further inaccuracies in computations. Errors **accumulate** over multiple calculations, affecting precision in scientific and financial applications.
![[Pasted image 20250309210816.png]]
### Handling Precision Issues in Large-Value Computations

##### Key Problems
- **Order of Addition Matters** → Different results can occur due to floating-point limitations.
- **Loss of Precision** → Small values may be ignored when added to large numbers.
- **Precision Limits** → Errors arise when numbers differ by $10^{16}$ or more.
##### Example: Order of Addition
**Incorrect (Large First)**
$(1,000,000,000 + 0.0001) + 5 = 1,000,000,005$
(Small value is lost)
**Correct (Small First)**
$(0.0001 + 5) + 1,000,000,000 = 1,000,000,005.0001$
(Preserves accuracy)
##### Best Practices
- **Add small values first**, then large ones.
- **Use higher precision types** (e.g., `double` or `long double`).
- **Avoid subtracting nearly equal large numbers**.